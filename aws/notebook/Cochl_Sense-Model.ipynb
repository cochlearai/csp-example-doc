{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Cochl.Sense Model Package from AWS Marketplace\n",
    "\n",
    "[Cochl.Sense](https://cochl.ai) is an AI-powered audio recognition API that analyzes audio and detects various sound events such as music, speech, sirens, alarms, and more.\n",
    "\n",
    "This notebook demonstrates how to deploy and use the Cochl.Sense model from AWS Marketplace using Amazon SageMaker.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Set up the environment](#2.-Set-up-the-environment)\n",
    "3. [Real-time Inference](#3.-Real-time-Inference)\n",
    "4. [Asynchronous Inference](#4.-Asynchronous-Inference)\n",
    "5. [Batch Transform](#5.-Batch-Transform)\n",
    "6. [Clean up](#6.-Clean-up)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "You can run this notebook one cell at a time by pressing Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package\n",
    "\n",
    "To subscribe to the model package:\n",
    "\n",
    "1. Open the model package listing page: **Cochl.Sense**\n",
    "2. On the AWS Marketplace listing, click on the **Continue to Subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agree with EULA, pricing, and support terms.\n",
    "4. Once you click on **Continue to configuration** button and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3.\n",
    "\n",
    "Copy the ARN corresponding to your region and specify it in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model package ARN from AWS Marketplace\n",
    "model_package_arn = \"<Customer to specify Model package ARN corresponding to their AWS region>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up the environment\n",
    "\n",
    "Install required packages and set up SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a deployable model from the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"cochl-sense-model-{int(time.time())}\"\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Real-time Inference\n\nReal-time inference is suitable for low-latency, interactive use cases. The maximum payload size is **25 MB** with a processing time limit of **60 seconds**.\n\n### Deploy the endpoint"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a real-time endpoint\n",
    "realtime_endpoint_name = f\"cochl-sense-realtime-{int(time.time())}\"\n",
    "instance_type = \"ml.g4dn.xlarge\"  # GPU instance for faster inference\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=realtime_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed: {realtime_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "\n",
    "The input must be raw audio binary data. Supported formats:\n",
    "- `audio/mp3`\n",
    "- `audio/wav`\n",
    "- `audio/ogg`\n",
    "- `application/octet-stream` (auto-detection)\n",
    "- `audio/x-raw; rate={sample_rate}; format={sample_format}; channels={num_channels}` (for raw PCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample audio file\n",
    "audio_file_path = \"data/sample.mp3\"\n",
    "content_type = \"audio/mp3\"\n",
    "\n",
    "with open(audio_file_path, \"rb\") as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "print(f\"Loaded audio file: {audio_file_path}\")\n",
    "print(f\"File size: {len(audio_data)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the endpoint using boto3 runtime client\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=realtime_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=audio_data\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke with sensitivity control (optional)\n",
    "\n",
    "You can adjust the detection sensitivity using `CustomAttributes`:\n",
    "\n",
    "- `X-Default-Sensitivity`: Default sensitivity for all tags. Range: [-2, 2] (integer). Default: 0\n",
    "- `X-Tags-Sensitivity`: Per-tag sensitivity adjustment as JSON string. Range: [-2, 2] (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke with custom sensitivity settings\n",
    "tags_sensitivity = json.dumps({\"Siren\": 2, \"Laughter\": -2})\n",
    "custom_attributes = f\"X-Default-Sensitivity=1,X-Tags-Sensitivity={tags_sensitivity}\"\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=realtime_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    CustomAttributes=custom_attributes,\n",
    "    Body=audio_data\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the real-time endpoint\n",
    "\n",
    "Delete the endpoint to avoid incurring charges when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the real-time endpoint\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_client.delete_endpoint(EndpointName=realtime_endpoint_name)\n",
    "print(f\"Endpoint deleted: {realtime_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Asynchronous Inference\n",
    "\n",
    "Asynchronous inference is suitable for large files or when you don't need immediate results. The maximum payload size is **1 GB** with a processing time limit of **60 minutes**.\n",
    "\n",
    "### Create an asynchronous inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "async_endpoint_name = f\"cochl-sense-async-{int(time.time())}\"\n",
    "async_output_path = f\"s3://{bucket}/cochl-sense/async-output/\"\n",
    "\n",
    "# Create async inference config\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=async_output_path,\n",
    "    max_concurrent_invocations_per_instance=4\n",
    ")\n",
    "\n",
    "# Deploy async endpoint\n",
    "async_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=async_endpoint_name,\n",
    "    async_inference_config=async_config\n",
    ")\n",
    "\n",
    "print(f\"Async endpoint deployed: {async_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload input file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload audio file to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_input_key = \"cochl-sense/async-input/sample.mp3\"\n",
    "s3_input_location = f\"s3://{bucket}/{s3_input_key}\"\n",
    "\n",
    "s3_client.upload_file(audio_file_path, bucket, s3_input_key)\n",
    "print(f\"Uploaded to: {s3_input_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke async endpoint\n",
    "response = runtime_client.invoke_endpoint_async(\n",
    "    EndpointName=async_endpoint_name,\n",
    "    InputLocation=s3_input_location,\n",
    "    ContentType=\"audio/mp3\"\n",
    ")\n",
    "\n",
    "output_location = response[\"OutputLocation\"]\n",
    "print(f\"Output will be stored at: {output_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for and retrieve the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll for the result\n",
    "import urllib.parse\n",
    "\n",
    "parsed_url = urllib.parse.urlparse(output_location)\n",
    "output_bucket = parsed_url.netloc\n",
    "output_key = parsed_url.path.lstrip(\"/\")\n",
    "\n",
    "max_retries = 60\n",
    "retry_interval = 5\n",
    "\n",
    "for i in range(max_retries):\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=output_bucket, Key=output_key)\n",
    "        result = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "        print(\"Inference completed!\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        break\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        print(f\"Waiting for result... ({i+1}/{max_retries})\")\n",
    "        time.sleep(retry_interval)\n",
    "else:\n",
    "    print(\"Timeout waiting for async inference result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the async endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=async_endpoint_name)\n",
    "print(f\"Async endpoint deleted: {async_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Transform\n",
    "\n",
    "Batch Transform is suitable for running inference on large datasets stored in S3.\n",
    "\n",
    "For datasets with mixed audio formats (mp3, wav, ogg), use `application/octet-stream` as the ContentType to enable auto-detection.\n",
    "\n",
    "### Upload batch input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample files for batch transform\n",
    "batch_input_prefix = \"cochl-sense/batch-input/\"\n",
    "batch_output_prefix = \"cochl-sense/batch-output/\"\n",
    "\n",
    "batch_input_path = f\"s3://{bucket}/{batch_input_prefix}\"\n",
    "batch_output_path = f\"s3://{bucket}/{batch_output_prefix}\"\n",
    "\n",
    "# Upload sample file\n",
    "s3_client.upload_file(audio_file_path, bucket, f\"{batch_input_prefix}sample.mp3\")\n",
    "print(f\"Batch input path: {batch_input_path}\")\n",
    "print(f\"Batch output path: {batch_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run a batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_job_name = f\"cochl-sense-batch-{int(time.time())}\"\n",
    "\n",
    "# Create batch transform job\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=batch_output_path,\n",
    "    max_payload=100,  # Max payload size in MB\n",
    "    max_concurrent_transforms=1,\n",
    ")\n",
    "\n",
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=batch_input_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/octet-stream\",  # Auto-detection for mixed formats\n",
    "    split_type=\"None\",\n",
    "    job_name=transform_job_name,\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "print(f\"Batch transform job started: {transform_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the transform job to complete\n",
    "print(\"Waiting for batch transform job to complete...\")\n",
    "\n",
    "waiter = sagemaker_client.get_waiter(\"transform_job_completed_or_stopped\")\n",
    "waiter.wait(TransformJobName=transform_job_name)\n",
    "\n",
    "# Check job status\n",
    "response = sagemaker_client.describe_transform_job(TransformJobName=transform_job_name)\n",
    "status = response[\"TransformJobStatus\"]\n",
    "print(f\"Transform job status: {status}\")\n",
    "\n",
    "if status == \"Completed\":\n",
    "    print(f\"Output available at: {batch_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve batch transform results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and display batch transform output files\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=batch_output_prefix)\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        print(f\"\\n=== {key} ===\")\n",
    "        output_obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        result = json.loads(output_obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "        print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(\"No output files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "Delete the model and clean up resources to avoid incurring unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "print(f\"Model deleted: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Clean up S3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to delete S3 data\n",
    "\n",
    "# # Delete async input/output\n",
    "# s3_client.delete_object(Bucket=bucket, Key=s3_input_key)\n",
    "\n",
    "# # Delete batch input/output\n",
    "# for prefix in [batch_input_prefix, batch_output_prefix]:\n",
    "#     response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "#     if \"Contents\" in response:\n",
    "#         for obj in response[\"Contents\"]:\n",
    "#             s3_client.delete_object(Bucket=bucket, Key=obj[\"Key\"])\n",
    "\n",
    "# print(\"S3 data cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsubscribe from the model package\n",
    "\n",
    "If you no longer need the model, you can unsubscribe from the AWS Marketplace:\n",
    "\n",
    "1. Navigate to **Machine Learning** tab in [Your Software subscriptions page](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_ind498)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then click **Cancel Subscription**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format Reference\n",
    "\n",
    "The inference result is returned in JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"metadata\": {\n",
    "    \"content_type\": \"audio/mp3\",\n",
    "    \"length_sec\": 3.012,\n",
    "    \"size_byte\": 12345,\n",
    "    \"name\": \"testfile.mp3\"\n",
    "  },\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"tags\": [\n",
    "        {\n",
    "          \"name\": \"Drum\",\n",
    "          \"probability\": 0.51982635\n",
    "        }\n",
    "      ],\n",
    "      \"start_time\": 0,\n",
    "      \"end_time\": 2\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "For available tags, see: https://docs.cochl.ai/sense/home/soundtags/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}