{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Cochl.Sense Model Package from AWS Marketplace\n",
    "\n",
    "[Cochl.Sense](https://cochl.ai) is an AI-powered audio recognition API that analyzes audio and detects various sound events such as music, speech, sirens, alarms, and more.\n",
    "\n",
    "This notebook demonstrates how to deploy and use the Cochl.Sense model from AWS Marketplace using Amazon SageMaker.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Set up the environment](#2.-Set-up-the-environment)\n",
    "3. [Real-time Inference](#3.-Real-time-Inference)\n",
    "4. [Asynchronous Inference](#4.-Asynchronous-Inference)\n",
    "5. [Batch Transform](#5.-Batch-Transform)\n",
    "6. [Clean up](#6.-Clean-up)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "You can run this notebook one cell at a time by pressing Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package\n",
    "\n",
    "To subscribe to the model package:\n",
    "\n",
    "1. Open the model package listing page: **Cochl.Sense**\n",
    "2. On the AWS Marketplace listing, click on the **Continue to Subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agree with EULA, pricing, and support terms.\n",
    "4. Once you click on **Continue to configuration** button and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3.\n",
    "\n",
    "Copy the ARN corresponding to your region and specify it in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up the environment\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This notebook requires Python 3.8+ and the following packages. If you're running this notebook locally (not on SageMaker), you may need to set up a virtual environment and install the required packages.\n",
    "\n",
    "#### Option 1: Using virtual environment (recommended for local development)\n",
    "\n",
    "```bash\n",
    "# Create and activate virtual environment\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n",
    "\n",
    "# Install required packages\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### Option 2: Install directly in notebook\n",
    "\n",
    "Run the cell below to install packages directly.\n",
    "\n",
    "#### AWS CLI Configuration (required for local development)\n",
    "\n",
    "If you're running this notebook locally, you need to configure AWS credentials:\n",
    "\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "You will be prompted to enter:\n",
    "- AWS Access Key ID\n",
    "- AWS Secret Access Key\n",
    "- Default region name (e.g., `us-east-1`)\n",
    "- Default output format (e.g., `json`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if packages are not installed (skip if running on SageMaker)\n",
    "# !pip install boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model package ARN from AWS Marketplace\n",
    "model_package_arn = \"<your-model-package-arn>\"  # Replace with your ARN from Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a deployable model from the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"cochl-sense-model-{int(time.time())}\"\n",
    "\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-time Inference\n",
    "\n",
    "Real-time inference is suitable for low-latency, interactive use cases. The maximum payload size is **25 MB** with a processing time limit of **60 seconds**.\n",
    "\n",
    "### Deploy the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a real-time endpoint\n",
    "realtime_endpoint_name = f\"cochl-sense-realtime-{int(time.time())}\"\n",
    "instance_type = \"ml.g4dn.xlarge\"  # GPU instance for faster inference\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=realtime_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed: {realtime_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Wait for the endpoint to be ready\n\nEndpoint deployment typically takes about **7 minutes**. You can check the endpoint status in the [SageMaker Console](https://console.aws.amazon.com/sagemaker/home#/endpoints). Proceed to the next step once the endpoint status is `InService`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "\n",
    "The input must be raw audio binary data. Supported formats:\n",
    "- `audio/mp3`\n",
    "- `audio/wav`\n",
    "- `audio/ogg`\n",
    "- `application/octet-stream` (auto-detection)\n",
    "- `audio/x-raw; rate={sample_rate}; format={sample_format}; channels={num_channels}` (for raw PCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample audio file\n",
    "audio_file_path = \"data/sample.mp3\"\n",
    "content_type = \"audio/mp3\"\n",
    "\n",
    "with open(audio_file_path, \"rb\") as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "print(f\"Loaded audio file: {audio_file_path}\")\n",
    "print(f\"File size: {len(audio_data)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the endpoint using boto3 runtime client\n",
    "runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=realtime_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=audio_data\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke with sensitivity control (optional)\n",
    "\n",
    "You can adjust the detection sensitivity using `CustomAttributes` as a JSON string:\n",
    "\n",
    "- `default_sensitivity`: Default sensitivity for all tags. Range: [-2, 2] (integer). Default: 0\n",
    "- `tags_sensitivity`: Per-tag sensitivity adjustment. Range: [-2, 2] (integer)\n",
    "\n",
    "Sensitivity can be set globally or individually per tag:\n",
    "- Positive values increase tag appearance (more tags will appear), negative values decrease it (fewer tags will appear).\n",
    "- If certain tags are not being detected frequently, try increasing the sensitivity.\n",
    "- If you experience too many false detections, lowering the sensitivity may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke with custom sensitivity settings\n",
    "custom_attributes = json.dumps({\n",
    "    \"default_sensitivity\": 1,\n",
    "    \"tags_sensitivity\": {\"Siren\": 2, \"Laughter\": -2}\n",
    "})\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=realtime_endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    CustomAttributes=custom_attributes,\n",
    "    Body=audio_data\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the real-time endpoint\n",
    "\n",
    "Delete the endpoint to avoid incurring charges when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the real-time endpoint\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_client.delete_endpoint(EndpointName=realtime_endpoint_name)\n",
    "print(f\"Endpoint deleted: {realtime_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Asynchronous Inference\n",
    "\n",
    "Asynchronous inference is suitable for large files or when you don't need immediate results. The maximum payload size is **1 GB** with a processing time limit of **60 minutes**.\n",
    "\n",
    "### Create an asynchronous inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sagemaker.async_inference import AsyncInferenceConfig\n\nasync_endpoint_name = f\"cochl-sense-async-{int(time.time())}\"\nasync_output_path = f\"s3://{bucket}/cochl-sense/async-output/\"\nasync_failure_path = f\"s3://{bucket}/cochl-sense/async-failure/\"\n\n# Create async inference config\nasync_config = AsyncInferenceConfig(\n    output_path=async_output_path,\n    failure_path=async_failure_path,\n    max_concurrent_invocations_per_instance=4\n)\n\n# Deploy async endpoint\nasync_predictor = model.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\",\n    endpoint_name=async_endpoint_name,\n    async_inference_config=async_config\n)\n\nprint(f\"Async endpoint deployed: {async_endpoint_name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Wait for the async endpoint to be ready\n\nEndpoint deployment typically takes about **7 minutes**. You can check the endpoint status in the [SageMaker Console](https://console.aws.amazon.com/sagemaker/home#/endpoints). Proceed to the next step once the endpoint status is `InService`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload input file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload audio file to S3\ns3_client = boto3.client(\"s3\")\nruntime_client = boto3.client(\"sagemaker-runtime\")\n\ns3_input_key = \"cochl-sense/async-input/sample.mp3\"\ns3_input_location = f\"s3://{bucket}/{s3_input_key}\"\n\ns3_client.upload_file(audio_file_path, bucket, s3_input_key)\nprint(f\"Uploaded to: {s3_input_location}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Invoke async endpoint\nresponse = runtime_client.invoke_endpoint_async(\n    EndpointName=async_endpoint_name,\n    InputLocation=s3_input_location,\n    ContentType=\"audio/mp3\",\n    Accept=\"application/json\"\n)\n\noutput_location = response[\"OutputLocation\"]\nprint(f\"Output will be stored at: {output_location}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for and retrieve the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Poll for the result\nimport urllib.parse\n\nparsed_url = urllib.parse.urlparse(output_location)\noutput_bucket = parsed_url.netloc\noutput_key = parsed_url.path.lstrip(\"/\")\n\nmax_retries = 60\nretry_interval = 5\n\nfor i in range(max_retries):\n    try:\n        obj = s3_client.get_object(Bucket=output_bucket, Key=output_key)\n        result = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n        print(\"Inference completed!\")\n        print(json.dumps(result, indent=2))\n        break\n    except s3_client.exceptions.NoSuchKey:\n        print(f\"Waiting for result... ({i+1}/{max_retries})\")\n        time.sleep(retry_interval)\nelse:\n    print(\"Timeout waiting for async inference result.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the async endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the async endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=async_endpoint_name)\n",
    "print(f\"Async endpoint deleted: {async_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Transform\n",
    "\n",
    "Batch Transform is suitable for running inference on large datasets stored in S3.\n",
    "\n",
    "For datasets with mixed audio formats (mp3, wav, ogg), use `application/octet-stream` as the ContentType to enable auto-detection.\n",
    "\n",
    "### Upload batch input data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload sample files for batch transform\ns3_client = boto3.client(\"s3\")\n\nbatch_input_prefix = \"cochl-sense/batch-input/\"\nbatch_output_prefix = \"cochl-sense/batch-output/\"\n\nbatch_input_path = f\"s3://{bucket}/{batch_input_prefix}\"\nbatch_output_path = f\"s3://{bucket}/{batch_output_prefix}\"\n\n# Upload sample file\ns3_client.upload_file(audio_file_path, bucket, f\"{batch_input_prefix}sample.mp3\")\nprint(f\"Batch input path: {batch_input_path}\")\nprint(f\"Batch output path: {batch_output_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run a batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_job_name = f\"cochl-sense-batch-{int(time.time())}\"\n",
    "\n",
    "# Create batch transform job\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=batch_output_path,\n",
    "    max_payload=100,  # Max payload size in MB\n",
    "    max_concurrent_transforms=1,\n",
    ")\n",
    "\n",
    "# Start the transform job\n",
    "transformer.transform(\n",
    "    data=batch_input_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/octet-stream\",  # Auto-detection for mixed formats\n",
    "    split_type=\"None\",\n",
    "    job_name=transform_job_name,\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "print(f\"Batch transform job started: {transform_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Monitor the batch transform job\n\nThe batch transform job typically takes about **7 minutes** to provision and start processing. You can check the job status in the [SageMaker Console](https://console.aws.amazon.com/sagemaker/home#/transform-jobs). Proceed to the next step once the job status is `Completed`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve batch transform results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and display batch transform output files\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=batch_output_prefix)\n",
    "\n",
    "if \"Contents\" in response:\n",
    "    for obj in response[\"Contents\"]:\n",
    "        key = obj[\"Key\"]\n",
    "        print(f\"\\n=== {key} ===\")\n",
    "        output_obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        result = json.loads(output_obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "        print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(\"No output files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "Delete the model and clean up resources to avoid incurring unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model\n",
    "sagemaker_client.delete_model(ModelName=model_name)\n",
    "print(f\"Model deleted: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Clean up S3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to delete S3 data\n",
    "\n",
    "# # Delete async input/output\n",
    "# s3_client.delete_object(Bucket=bucket, Key=s3_input_key)\n",
    "\n",
    "# # Delete batch input/output\n",
    "# for prefix in [batch_input_prefix, batch_output_prefix]:\n",
    "#     response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "#     if \"Contents\" in response:\n",
    "#         for obj in response[\"Contents\"]:\n",
    "#             s3_client.delete_object(Bucket=bucket, Key=obj[\"Key\"])\n",
    "\n",
    "# print(\"S3 data cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsubscribe from the model package\n",
    "\n",
    "If you no longer need the model, you can unsubscribe from the AWS Marketplace:\n",
    "\n",
    "1. Navigate to **Machine Learning** tab in [Your Software subscriptions page](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_ind498)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then click **Cancel Subscription**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format Reference\n",
    "\n",
    "The inference result is returned in JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"metadata\": {\n",
    "    \"content_type\": \"audio/mp3\",\n",
    "    \"length_sec\": 3.012,\n",
    "    \"size_byte\": 12345,\n",
    "    \"name\": \"testfile.mp3\"\n",
    "  },\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"tags\": [\n",
    "        {\n",
    "          \"name\": \"Drum\",\n",
    "          \"probability\": 0.51982635\n",
    "        }\n",
    "      ],\n",
    "      \"start_time\": 0,\n",
    "      \"end_time\": 2\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "For available tags, see: https://docs.cochl.ai/sense/home/soundtags/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}